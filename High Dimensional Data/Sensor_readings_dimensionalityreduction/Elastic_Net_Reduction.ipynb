{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee057cc",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff82b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV, lasso_path, ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d33ae",
   "metadata": {},
   "source": [
    "In this question, I will, as instructed, be testing different forms of regularization and its effect on regression. For this specific question, we will be regressing index of democracy on various factors, such as GDP per capita, population size, age breakdown of population, and average years of education of the population. The final goal is to ultimately see, with regularization, if there is a true relationship between GDP per capita and democracy, with other variables included to soak up standard error and reduce any potential omission bias, with the hypothesis being that these other variables, which are not necessarily the main factors of study, are assumed to potentially be correlated with democracy index. While standard regression is almost forced to assign a coefficient to each regressor, these regularization methods allow the model to deflate the coefficients, or even reduce to 0 depending on the model, thus shining more light on which variables might truly be the most impactful in terms of affecting the dependent variable. Thus, with these methods and other regressors, I hope to see, while practicing various regularization methods, if GDP per capita truly is the most important factor, or is steadily included with a non-zero coefficient for all modeling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588e0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"../income_democracy.csv\")\n",
    "scaler = StandardScaler()\n",
    "df = df.dropna()\n",
    "yy = df['dem_ind']\n",
    "\n",
    "df = df.drop(['code','country','year','dem_ind'],axis=1)\n",
    "\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,yy,random_state=1993, train_size=.8, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e455512",
   "metadata": {},
   "source": [
    "In the above cell, I load the data, drop the columns we are told to drop in Piazza, drop any rows with null data, scale the data (which is a necessary step for regularization methods), and finally split the data into train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2da98",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bf686",
   "metadata": {},
   "source": [
    "As stated in lecture, ridge regression serves to reduce the coefficients. While it cannot perform variable selection, ridge still serves to decrease final variance by adding some bias into the model, and relaxes the assumption with standard linear regression that regressors must be unbiased -- with ridge, we can use biased regressors, and with such will get better results than standard regression. Furthermore, it helps best in situations in which many regressors have similar-magnitude impacts on a dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2019b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = list(np.arange(1,50,.1)), cv=10).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d4bc4",
   "metadata": {},
   "source": [
    "Above, I use the all-in-one function from sklearn called RidgeCV, and test various values of alpha from 1 to 50, in 0.1 intervals, via cross-validation. I then fit the model with the training data, which then tunes the alpha value as it trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484f74a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Optimal Alpha for Ridge: 39.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Optimal Alpha for Ridge: {round(ridge.alpha_,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7949c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Test Prediction for Ridge: 0.054\n"
     ]
    }
   ],
   "source": [
    "ridge_preds = ridge.predict(X = x_test)\n",
    "print(f\"MSE for Test Prediction for Ridge: {round(mean_squared_error(y_test,ridge_preds),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639183ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table includes the coefficients for the Regressors with Ridge Regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_gdppc</th>\n",
       "      <td>0.099319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_pop</th>\n",
       "      <td>-0.011793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_1</th>\n",
       "      <td>-0.022159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_2</th>\n",
       "      <td>0.003654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_3</th>\n",
       "      <td>-0.020758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_4</th>\n",
       "      <td>0.028502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_5</th>\n",
       "      <td>0.030803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>educ</th>\n",
       "      <td>0.079497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_median</th>\n",
       "      <td>0.016462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficient\n",
       "Variable              \n",
       "log_gdppc     0.099319\n",
       "log_pop      -0.011793\n",
       "age_1        -0.022159\n",
       "age_2         0.003654\n",
       "age_3        -0.020758\n",
       "age_4         0.028502\n",
       "age_5         0.030803\n",
       "educ          0.079497\n",
       "age_median    0.016462"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_ridge = pd.DataFrame([df.columns,ridge.coef_])\n",
    "print(\"The following table includes the coefficients for the Regressors with Ridge Regression\")\n",
    "coefs_ridge.T.rename({0:'Variable',1:'Coefficient'},axis=1).set_index('Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ed0df",
   "metadata": {},
   "source": [
    "Looking at the coefficients, we can see that the log-GDP has one of the lowest coefficients in absolute value across all coefficients, which is interesting. Notice also how age median is not given a coefficient, perhaps due to collinearity issues with the other age variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4f1c7",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628c639",
   "metadata": {},
   "source": [
    "Now, moving on to Lasso: Lasso is best at variable selection, in that it is able to reduce certain coefficients to 0 due to its penalty term. It  also adds a bit of bias into the model in order to reduce variance, and allows for using regressors that might be correlated, as Lasso reduces the coefficients in a way to account for any correlation or possible covariates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6fda769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Optimal Alpha for Lasso: 0.006\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(alphas = list(np.arange(0.001,2,.001)), cv=10, max_iter=10000).fit(x_train,y_train)\n",
    "print(f\"Final Optimal Alpha for Lasso: {round(lasso.alpha_,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ef0a6",
   "metadata": {},
   "source": [
    "Again, above I use the all-in-one cross-validation function for Lasso, in which I try various levels of alpha, ranging from .001 to 2, in intervals of .001. The optimal alpha is printed above, and was used in final fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a7296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Test Prediction for Lasso: 0.054\n"
     ]
    }
   ],
   "source": [
    "lasso_preds = lasso.predict(X = x_test)\n",
    "print(f\"MSE for Test Prediction for Lasso: {round(mean_squared_error(y_test,lasso_preds),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb33392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table includes the coefficients for the Regressors with Lasso Regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_gdppc</th>\n",
       "      <td>0.112588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_pop</th>\n",
       "      <td>-0.005378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_1</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_2</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_3</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_4</th>\n",
       "      <td>0.021004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_5</th>\n",
       "      <td>0.042281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>educ</th>\n",
       "      <td>0.07971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_median</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficient\n",
       "Variable              \n",
       "log_gdppc     0.112588\n",
       "log_pop      -0.005378\n",
       "age_1             -0.0\n",
       "age_2             -0.0\n",
       "age_3             -0.0\n",
       "age_4         0.021004\n",
       "age_5         0.042281\n",
       "educ           0.07971\n",
       "age_median         0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_lasso = pd.DataFrame([df.columns,lasso.coef_])\n",
    "print(\"The following table includes the coefficients for the Regressors with Lasso Regression\")\n",
    "coefs_lasso.T.rename({0:'Variable',1:'Coefficient'},axis=1).set_index('Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95551c7a",
   "metadata": {},
   "source": [
    "Notice first how, with Lasso, we do have four variables whose coefficients drop to 0: Ages1 to 3 and Age_median! Thus, it shows that perhaps a lot of the age columns were redundant, and now GDP has the largets absolute effect! The MSE on test set is nearly identical to that of ridge. \n",
    "\n",
    "HOWEVER, one flaw with Lasso is that, if there ARE covariates or correlated variables in the regressors, Lasso does NOT necessarily have a reason for \"choosing\" one variable over the other, and thus might not always tell the \"correct\" story. Thus, it is not clear if ages 1-3 or age_median are truly the redundant variables, or if they, in place of age 4 and 5, would be \"better predictors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b526917",
   "metadata": {},
   "source": [
    "# Adaptive Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a893745",
   "metadata": {},
   "source": [
    "Adaptive Lasso is a version of lasso regularization, but in which the parameters are weighted, thus reducing a common effect of regular LASSO in which larger coefficients are shrunk to a larger extent than smaller coefficients, thus overly biasing the model. With adaptive lasso, the larger coefficients are reduced less than their regular lasso counterparts, compared to regressors with smaller coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da9ee157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Lasso Optimal lambda = 6.137342527177675e-05\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "p=len(x_train.columns)\n",
    "ridge_betas = RidgeCV(\n",
    "    fit_intercept=True, alphas=np.logspace(-6, 6, num=100)).fit(x_train, y_train).coef_\n",
    "w_ridge = ridge_betas**-gamma\n",
    "X_ridge = x_train/w_ridge\n",
    "lambdas, lasso_betas, _ = lasso_path(X_ridge, y_train)\n",
    "\n",
    "lasso_betas = lasso_betas/w_ridge[:, None]\n",
    "lasso_coef = pd.DataFrame(index=lambdas, data=lasso_betas.T)\n",
    "lasso_coef.columns = [f'B{i}' for i in range(1, p+1)]\n",
    "non_zero = lasso_coef.abs().mean() > 1e-1\n",
    "lasso_coef = lasso_coef.loc[:, non_zero]\n",
    "\n",
    "lassoCV = LassoCV(alphas=lambdas, fit_intercept=True, cv=10)\n",
    "lassoCV.fit(X_ridge, y_train)  \n",
    "lassoMSEs = pd.Series(lassoCV.mse_path_.mean(1), index=lambdas)\n",
    "lambda_lasso = lassoMSEs.idxmin()\n",
    "lasso_score = np.mean((lassoCV.predict(X_ridge)-y_train)**2)\n",
    "print(f'Adaptive Lasso Optimal lambda = {lambda_lasso}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d0a2c",
   "metadata": {},
   "source": [
    "In the above cell, I set up the adaptive lasso algorithm as given in the sample code, this time with gamma = 1. Please see print-out for optimal Lambda value.\n",
    "\n",
    "After setting up and fitting the adaptive lasso model, I then test it on test data below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d958b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Test Prediction for ADAPTIVE Lasso: 0.053\n"
     ]
    }
   ],
   "source": [
    "X_ridge_test = x_test/w_ridge\n",
    "adaptive_preds = lassoCV.predict(X_ridge_test)\n",
    "print(f\"MSE for Test Prediction for ADAPTIVE Lasso: {round(mean_squared_error(y_test,adaptive_preds),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc62d64",
   "metadata": {},
   "source": [
    "As can be seen, the MSE went down ever so slight with adaptive Lasso vs regular Lasso (by .003 points, which is very minor, if even significant). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475a7b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table includes the coefficients for the Regressors with ADAPTIVE Lasso Regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_gdppc</th>\n",
       "      <td>1.174748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_pop</th>\n",
       "      <td>0.409131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_3</th>\n",
       "      <td>0.599415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_4</th>\n",
       "      <td>1.034946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_5</th>\n",
       "      <td>1.202417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>educ</th>\n",
       "      <td>1.069954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_median</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficient\n",
       "Variable              \n",
       "log_gdppc     1.174748\n",
       "log_pop       0.409131\n",
       "age_1              0.0\n",
       "age_2              0.0\n",
       "age_3         0.599415\n",
       "age_4         1.034946\n",
       "age_5         1.202417\n",
       "educ          1.069954\n",
       "age_median         0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_adaptive = pd.DataFrame([df.columns,lassoCV.coef_])\n",
    "print(\"The following table includes the coefficients for the Regressors with ADAPTIVE Lasso Regression\")\n",
    "coefs_adaptive.T.rename({0:'Variable',1:'Coefficient'},axis=1).set_index('Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdc973",
   "metadata": {},
   "source": [
    "As can be seen, with adaptive lasso, only 3 coefficients got reduced to 0: Ages 1, 4, and 5 are now with 0-coefficient, and age_median and age_2 show up, which could be due to how lasso sometimes randomly decides which coefficients to reduce to 0 and which to keep. Oddly enough, in this run, age_3 and age_median have the highest coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe202a",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdbfe7a",
   "metadata": {},
   "source": [
    "As the final model, I will run an elastic net model, which is a form of regularization in which there is a balanced L1 and L2 penalty term (A fraction of penalty term is from L1, and the other fraction from L2). Thus, with elastic net, not only do we have to tune lambda, but also a new parameter, which serves as the ratio of L1/L2 punishment. \n",
    "\n",
    "Luckily, there is a handy-dandy function in sklearn that does the dual optimization for me using cross-validation: ElasticNetCV. I pass in a list of possible values for the lambda term and the l1-ratio term, which I picked based on prior runs to reduce run-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf866283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda for Elastic Net: 0.041 ... Optimal l1/l2 penalization ratio: 0.12\n"
     ]
    }
   ],
   "source": [
    "elastic = ElasticNetCV(l1_ratio = list(np.arange(0.1,1.001,.01)), alphas = list(np.arange(.001,1,.001)), cv=10, max_iter=100000).fit(x_train,y_train)\n",
    "print(f\"Optimal lambda for Elastic Net: {elastic.alpha_} ... Optimal l1/l2 penalization ratio: {round(elastic.l1_ratio_,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6d85e",
   "metadata": {},
   "source": [
    "Notice that the ratio is higher-weighted toward l2, meaning that the model benefits from higher ridge-like penalty than lasso! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab6325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Test Prediction for Elastic Net: 0.054\n"
     ]
    }
   ],
   "source": [
    "elastic_preds = elastic.predict(X = x_test)\n",
    "print(f\"MSE for Test Prediction for Elastic Net: {round(mean_squared_error(y_test,elastic_preds),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac98555",
   "metadata": {},
   "source": [
    "This model, has a similar MSE as ridge and lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f34f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table includes the coefficients for the Regressors with Elastic Net\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_gdppc</th>\n",
       "      <td>0.105273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_pop</th>\n",
       "      <td>-0.006922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_1</th>\n",
       "      <td>-0.000783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_3</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_4</th>\n",
       "      <td>0.026351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_5</th>\n",
       "      <td>0.041927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>educ</th>\n",
       "      <td>0.079991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_median</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficient\n",
       "Variable              \n",
       "log_gdppc     0.105273\n",
       "log_pop      -0.006922\n",
       "age_1        -0.000783\n",
       "age_2              0.0\n",
       "age_3             -0.0\n",
       "age_4         0.026351\n",
       "age_5         0.041927\n",
       "educ          0.079991\n",
       "age_median         0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_elastic = pd.DataFrame([df.columns,elastic.coef_])\n",
    "print(\"The following table includes the coefficients for the Regressors with Elastic Net\")\n",
    "coefs_elastic.T.rename({0:'Variable',1:'Coefficient'},axis=1).set_index('Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac9c89",
   "metadata": {},
   "source": [
    "Finally, looking at the coefficients, we can see again that some age variables get reduced to 0 (age 2,3, and median), while gdp again has the highest absolute value coefficient of all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf0e17",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768206f",
   "metadata": {},
   "source": [
    "In conclusion, each model seems to have its advantages and disadvantages, and thus to answer the question of which model I would ultimately choose, I will approach that question in 2 ways: (1) the model that ultimately seems to best predict democracy coefficient, and then (2) the model that might have the best indicator of the effect of GDP on democracy (which might arguably be the same model). \n",
    "\n",
    "\n",
    "In terms of simply picking the best model, My criteria for picking a model are that the model obviously performs well, and also, when performance is similar, I would pick the simpler model, or that which ultimately has less regressors with non-zero coefficients. \n",
    "\n",
    "Thus, in terms of model accuracy, all models perform nearly identically, with adaptive lasso being the only model that gets a slightly better MSE. Thus, since performance is similar across all methods, I would choose a lasso-based method, due to the their simplicity. Since basic lasso was able to 0-out 4 variables, I might pick it as my final model just in terms of accuracy + simplicity.\n",
    "\n",
    "However, given that **adaptive lasso has the best MSE, and still pushes 3 variables to 0-coefficient, I might choose adaptive lasso as the best model overall**. Furthermore, the concept of adaptive lasso lends somewhat better to the task at hand, which is ultimately understanding the effect that GDP has on democracy. With regular lasso, bigger coefficients get reduced more in magnitude, and thus with adaptive, we get a better sense of perhaps the \"true\" effect of a variable, giving more \"explanatory power\" to the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
