full_join(., median_list_price_df, by=join_cols) %>%
full_join(., median_sale_price_df, by=join_cols) %>%
full_join(., median_sale_to_list_ratio_df, by=join_cols) %>%
full_join(., prct_homes_above_list_df, by=join_cols)
head(main_df, 5)
#bring in the uscities location data, and concatinate the city / state columns to match
#the formatting of the main_df
location_df <- read.csv('./data/uscities.csv') %>%
select(city, state_id, lat, lng) %>%
unite(RegionName, c('city', 'state_id'), sep = ', ')
main_df <- left_join(main_df, location_df, by='RegionName')
main_df$month <- as.Date(main_df$month, format = "X%m.%d.%Y")
head(main_df, 5)
write.csv(main_df, './data/formatted_df.csv')
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
#contains the number of homes for sale
inventory_for_sale_df <- read.csv('./data/inventory_for_sale.csv', header=TRUE)
#contains the median list prices of homes for a region
median_list_price_df <- read.csv('./data/median_list_price.csv', header=TRUE)
#contains median sale prices of homes for a region
median_sale_price_df <- read.csv('./data/median_sale_price.csv', header=TRUE)
#contains median sale price to list price ratio of homes for a region
median_sale_to_list_ratio_df <- read.csv('./data/median_sale_to_list_ratio.csv', header=TRUE)
#contains percent of homes sold above list price for a region
prct_homes_above_list_df <- read.csv('./data/prct_homes_above_list.csv', header=TRUE)
#contains the Zillow Home Value Index (ZHVI) for a region. This value is calculated by Zillow to estimate the value of a home within the 35th to 65th percentile range
zhvi_df <- read.csv('./data/ZHVI.csv', header=TRUE)
head(inventory_for_sale_df, 5)
inventory_for_sale_df <- inventory_for_sale_df %>%
gather(month, inventory, -c(RegionID, SizeRank, RegionName, RegionType, StateName))
median_list_price_df <- median_list_price_df %>%
gather(month, median_list_price, -c(RegionID, SizeRank, RegionName, RegionType, StateName))
median_sale_price_df <- median_sale_price_df %>%
gather(month, median_sale_price, -c(RegionID, SizeRank, RegionName, RegionType, StateName))
median_sale_to_list_ratio_df <- median_sale_to_list_ratio_df %>%
gather(month, StL_ratio, -c(RegionID, SizeRank, RegionName, RegionType, StateName))
prct_homes_above_list_df <- prct_homes_above_list_df %>%
gather(month, prct_above_list, -c(RegionID, SizeRank, RegionName, RegionType, StateName))
zhvi_df <- zhvi_df %>%
gather(month, ZVHI, -c(RegionID, SizeRank, RegionName, RegionType, StateName))
join_cols <- c('RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'month')
main_df <- full_join(zhvi_df, inventory_for_sale_df, by=join_cols) %>%
full_join(., median_list_price_df, by=join_cols) %>%
full_join(., median_sale_price_df, by=join_cols) %>%
full_join(., median_sale_to_list_ratio_df, by=join_cols) %>%
full_join(., prct_homes_above_list_df, by=join_cols)
head(main_df, 5)
#bring in the uscities location data, and concatinate the city / state columns to match
#the formatting of the main_df
location_df <- read.csv('./data/uscities.csv') %>%
select(city, state_id, lat, lng) %>%
unite(RegionName, c('city', 'state_id'), sep = ', ')
main_df <- left_join(main_df, location_df, by='RegionName')
main_df$month <- as.Date(main_df$month, format = "X%m.%d.%Y")
head(main_df, 5)
write.csv(main_df, './data/formatted_df.csv')
knitr::opts_chunk$set(echo = TRUE)
# Clear the environment
rm(list = ls())
# Set a random seed for code reproducibility
set.seed(25)
# Install required libraries
#install.packages("tidyr", repos = "http://cran.us.r-project.org")
#library(tidyr)
# Read the formatted csv file
data <- read.csv(file = "../../Data/prelim_data_cleaning.csv")
# View the structure of the data
str(data)
# Read the formatted csv file
data <- read.csv(file = "../../Data/prelim_data_cleaning.csv")
# View the structure of the data
str(data)
# Drop columns "X.1", "X", and "ts_month"
data <- subset(data, select = -c(X.1, X, ts_month))
str(data)
# Create a list of column names
col_names <- colnames(data)
# Define a function to print the percentage of missing values for variables that have missing values
missing_values_pct <- function(my_data, my_list) {
for (each_item in my_list) {
num_na <- sum(is.na(my_data[, each_item]))
if (num_na > 0) {
pct_mv <- round((num_na / nrow(my_data)) * 100, 2)
cat(each_item, ": ", pct_mv, '%', '\n')
}
}
}
# Call the missing_values_pct function
missing_values_pct(data, col_names)
# Method 1: Drop all rows with missing values
data_1 <- data[complete.cases(data), ]
str(data_1)
# Check missing values for data_1
col_names_1 <- colnames(data_1)
missing_values_pct(data_1, col_names_1)
# Method 2: Drop all rows with missing values for the two response variables
data_2 <- data[!is.na(data$prct_above_list), ]
str(data_2)
# Method 3: Change date to 2020-2022 and then drop all missing values
data_3 <- data[data$ts_month >= '2020-01-01', ]
str(data_3)
# Check missing values for data_3
col_names_3 <- colnames(data_3)
missing_values_pct(data_3, col_names_3)
# Drop all missing values
data_3 <- data_3[complete.cases(data_3), ]
str(data_3)
# Check missing values for data_3 after dropping all missing values
col_names_3 <- colnames(data_3)
missing_values_pct(data_3, col_names_3)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(car)
library(tidyverse)
library(broom)
library(gridExtra)
# Reading the formatted dataframe
df <- read.csv("../Data/datacleaning_plus_addedfeatures.csv")
# Reading the formatted dataframe
df <- read.csv("../../Data/datacleaning_plus_addedfeatures.csv")
head(df)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(car)
library(tidyverse)
library(broom)
library(gridExtra)
# Reading the formatted dataframe
df <- read.csv("../../Data/datacleaning_plus_addedfeatures.csv")
head(df)
standard_model = lm(1/(y+1)~log(SizeRank)*log(ZVHI)+log(inventory)+as.factor(season)+as.factor(loc_cluster)+as.factor(year), data=df)
summary(standard_model)
model.diag.metrics <- augment(standard_model)
head(model.diag.metrics)
residplot1 <- ggplot(model.diag.metrics, aes(`log(ZVHI)`, `1/(y + 1)`)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = `log(ZVHI)`, yend = .fitted), color = "red", size = 0.1, alpha = 0.2)
residplot2 <- ggplot(model.diag.metrics, aes(`log(SizeRank)`, `1/(y + 1)`)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = `log(SizeRank)`, yend = .fitted), color = "red", size = 0.1, alpha = 0.2)
residplot3 <- ggplot(model.diag.metrics, aes(`log(inventory)`, `1/(y + 1)`)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = `log(inventory)`, yend = .fitted), color = "red", size = 0.1, alpha = 0.2)
grid.arrange(residplot1, residplot2, residplot3, nrow = 2, ncol = 2)
par(mfrow = c(2, 2))
plot(standard_model)
par(mfrow = c(1, 2))
plot(standard_model, 4, id.n = 5)
plot(standard_model, 5)
model.diag.metrics %>%
top_n(5, wt = .cooksd)
knitr::opts_chunk$set(echo = TRUE)
# Clear the environment
rm(list = ls())
# Set a random seed for code reproducibility
set.seed(25)
# Install required libraries
install.packages("xts", repos = "http://cran.us.r-project.org")
library(xts)
install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(lubridate)
library(ggplot2)
# Read the formatted csv file
data <- read.csv(file = "../0_Data Collection and Formatting/data/formatted_df.csv")
# View the structure of the data
str(data)
# Read the formatted csv file
data <- read.csv(file = "../0_Data Collection and Formatting/data/formatted_df.csv")
# View the structure of the data
str(data)
# Convert RegionID from int to chr
data$RegionID <- as.character(data$RegionID)
typeof(data$RegionID)
# Create a new variable, ts_month, with data type "Date"
data$ts_month <- ymd(data$month)
# View the dataframe with ts_month
data <- data[order(data$ts_month), ]
str(data)
# Create a list of column names
col_names <- colnames(data)
# Define a function to print the number of missing values for all variables
missing_values <- function(my_data, my_list) {
for (each_item in my_list) {
num_na <- sum(is.na(my_data[, each_item]))
cat(each_item, ": ", num_na, '\n')
}
}
# Call the missing_values function
missing_values(data, col_names)
# Define a function to print the percentage of missing values for variables that have missing values
missing_values_pct <- function(my_data, my_list) {
for (each_item in my_list) {
num_na <- sum(is.na(my_data[, each_item]))
if (num_na > 0) {
pct_mv <- round((num_na / nrow(my_data)) * 100, 2)
cat(each_item, ": ", pct_mv, '%', '\n')
}
}
}
# Call the missing_values_pct function
missing_values_pct(data, col_names)
# Make a copy of data
temp_data <- data.frame(data)
# Create a new column in temp_data for total missing values
temp_data$total_mv <- vector(mode="integer", length=nrow(temp_data))
# Define a function that adds 1 to a number
plus_one <- function(x) {
x + 1
}
# Define a function that sums the number of total missing values per row
sum_mv <- function(my_data, col_names) {
index_tracker <- 1
for (each_row in 1:nrow(my_data)) {
total_mv <- sum(is.na(my_data[each_row, col_names]))
my_data$total_mv[index_tracker] <- total_mv
index_tracker <- plus_one(index_tracker)
}
return(my_data)
}
# Call sum_mv function
temp_data <- sum_mv(temp_data, col_names)
str(temp_data)
# Plot date vs. missing values to see where we're missing the most values
ggplot(data = temp_data, aes(x=ts_month, y=total_mv)) + geom_bar(stat='identity')
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
suppressMessages(library(mapview))
suppressMessages(library(randomForest))
suppressMessages(library(corrplot))
suppressMessages(library(RColorBrewer))
suppressMessages(library("PerformanceAnalytics"))
suppressMessages(library(car))
suppressMessages(library(regclass))
suppressMessages(library(e1071))
suppressMessages(library(caret))
set.seed(1993)
df <- read.csv("../../Data/data_cleaning.csv")
train_ind = sample(seq(nrow(df)), 0.8*nrow(df), replace = FALSE, prob = NULL)
df$y = df$StL_ratio*df$prct_above_list
df$year = as.integer(substr(df$month,0,4))
df$month = as.integer(substr(df$month,6,7))
df = df %>%
mutate(season = ifelse(month >= 3 & month <= 5,"Spring",ifelse(month >= 6 & month <= 8,"Summer",ifelse(month >= 9& month <= 11,"Fall","Winter"))))
mapview(df,xcol="lng",ycol="lat",zcol="y",crs=4269)
n_clusters = 8
kmeans_loc_cluster = kmeans(select(df,c("lat","lng")),n_clusters)
centers = data.frame(kmeans_loc_cluster$centers)
#write.csv(centers,"../Data/cluster_centers.csv")
df$loc_cluster=kmeans_loc_cluster$cluster
mapview(as.data.frame(kmeans_loc_cluster$centers),xcol="lng",ycol="lat",crs=4269)
vars = select(df,c('SizeRank','ZVHI','inventory','median_list_price','median_sale_price','year','loc_cluster'))
M <- cor(vars)
corrplot(M, type="upper", order="hclust",
col=brewer.pal(n=8, name="RdYlBu"))
chart.Correlation(vars, histogram=TRUE, pch=19)
#write.csv(df,"../Data/datacleaning_plus_addedfeatures.csv")
train <- df[train_ind,]
test <- df[-train_ind,]
summary(lm(y~as.factor(loc_cluster)+as.factor(year),data=train))
standard_model = lm(1/(y+1)~log(SizeRank)*log(ZVHI)+log(inventory)+as.factor(season)+as.factor(loc_cluster)+as.factor(year), data=train)
standard_preds = predict(standard_model,test)
summary(standard_model)
plot(standard_model)
durbinWatsonTest(standard_model)
cooksD <- cooks.distance(standard_model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
names_of_influential <- names(influential)
outliers <- df[names_of_influential,]
without_outliers <- df %>% anti_join(outliers)
model_nooutliers <- lm(1/(y+1)~log(SizeRank)*log(ZVHI)+log(inventory)+as.factor(season)+as.factor(loc_cluster)+as.factor(year), data = without_outliers)
summary(model_nooutliers)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
df <- read.csv('./data/datacleaning_plus_addedfeatures.csv', header=TRUE)
#build test/train dataset
set.seed(123)
sample <- sample.int(n = nrow(df), size = floor(.70*nrow(df)), replace = F)
train_df <- df[sample, ]
test_df <- df[-sample, ]
#scale the dataset
train_df[c(4, 8, 9, 10, 11, 12, 14,15,16, 18, 20)] <- scale(train_df[c(4, 8, 9, 10, 11, 12, 14,15,16, 18, 20)])
test_df[c(4, 8, 9, 10, 11, 12, 14,15,16, 18, 20)] <- scale(test_df[c(4, 8, 9, 10, 11, 12, 14,15,16, 18, 20)])
mlr_mdl <- lm(y ~ SizeRank + ZVHI + inventory + year + season + loc_cluster, data = train_df)
#predict values on the 'test' dataset
test_df$mlr_prediction <- predict(mlr_mdl, test_df)
#calculate the RMSE of this model
mlr_RMSE <- sqrt(
sum((test_df$y - test_df$mlr_prediction)^2) / nrow(test_df)
)
print(mlr_RMSE)
log_lin_mdl <- lm(log(y+1) ~ SizeRank + ZVHI + inventory + year + season + loc_cluster, data = train_df)
#predict values on the 'test' dataset
test_df$log_lin_prediction <- predict(log_lin_mdl, test_df)
#calculate the RMSE of this model
log_lin_RMSE <- sqrt(
sum((test_df$y - test_df$log_lin_prediction)^2) / nrow(test_df)
)
print("RMSE of log-linear transformation: ")
print(log_lin_RMSE)
library(pls)
pca_mdl <- pcr(y ~ SizeRank + ZVHI + inventory + year + season + loc_cluster, data = train_df, scale=TRUE, validation="CV")
summary(pca_mdl)
validationplot(pca_mdl)
test_df$pca_pred <- predict(pca_mdl, test_df, ncomp = 5)
PCA_RMSE <- sqrt(
sum((test_df$y - test_df$pca_pred)^2) / nrow(test_df)
)
print(PCA_RMSE)
library(glmnet)
predictors <- data.matrix(train_df[, c('SizeRank', 'ZVHI', 'inventory', 'year', 'season', 'loc_cluster')])
responses <- train_df$y
LASSO_mdl <- cv.glmnet(predictors, responses, alpha = 1)
plot(LASSO_mdl)
best_lambda <- LASSO_mdl$lambda.min
print("The best lambda value for our LASSO Regression model is: ")
print(best_lambda)
test_df$LASSO_pred <- predict (LASSO_mdl, s=best_lambda, newx = data.matrix(test_df[, c('SizeRank', 'ZVHI', 'inventory', 'year', 'season', 'loc_cluster')]))
LASSO_RMSE <- sqrt(
sum((test_df$y - test_df$LASSO_pred)^2) / nrow(test_df)
)
print(LASSO_RMSE)
#forward stepwise regression
intercept_mdl <- lm(y~1, data=train_df)
all_mdl <- lm(y ~ SizeRank + ZVHI + inventory + year + season + loc_cluster, data=train_df)
forward_step <- step(intercept_mdl, direction='forward', scope=formula(all_mdl), trace=0)
forward_step$anova
forward_step$coefficients
#forward stepwise regression
backward_step <- step(all_mdl, direction='backward', scope=formula(all_mdl), trace=0)
backward_step$anova
backward_step$coefficients
test_df$fwd_prediction <- predict(forward_step, test_df)
test_df$bkw_prediction <- predict(backward_step, test_df)
fwd_RMSE <- sqrt(
sum((test_df$y - test_df$fwd_prediction)^2) / nrow(test_df)
)
bkw_RMSE <- sqrt(
sum((test_df$y - test_df$bkw_prediction)^2) / nrow(test_df)
)
print("The RMSE from the forward stepwise regression is:")
print(fwd_RMSE)
print("The RMSE from the backward stepwise regression is:")
print(bkw_RMSE)
test_df$fwd_prediction <- predict(forward_step, test_df)
test_df$bkw_prediction <- predict(backward_step, test_df)
fwd_RMSE <- sqrt(
sum((test_df$y - test_df$fwd_prediction)^2) / nrow(test_df)
)
bkw_RMSE <- sqrt(
sum((test_df$y - test_df$bkw_prediction)^2) / nrow(test_df)
)
print("The RMSE from the forward stepwise regression is:")
print(fwd_RMSE)
print("The RMSE from the backward stepwise regression is:")
print(bkw_RMSE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
df <- read.csv('../../Data/datacleaning_plus_addedfeatures.csv', header=TRUE)
df$season = as.factor(df$season)
df$loc_cluster = as.factor(df$loc_cluster)
dummies <- dummyVars(data = df, y ~ SizeRank + ZVHI + inventory + year + season + loc_cluster)
df <- cbind(as.data.frame(df$y), predict(dummies, df))
colnames(df)[1] <- "y"
model <- knnreg(data = df, y ~ .)
model
model <- train(
data = df,
y ~ .,
method = 'knn',
use.all = FALSE
)
library(dplyr)
library(tidyverse)
library(mapview)
library(randomForest)
library(corrplot)
library(RColorBrewer)
library("PerformanceAnalytics")
library(car)
library(regclass)
library(e1071)
library(caret)
set.seed(1993)
df = read.csv("../../Data/datacleaning_plus_addedfeatures.csv")
svm_df = df %>% select("y","SizeRank","ZVHI","inventory","season","loc_cluster","year")
svm_to_scale = svm_df %>% select("SizeRank","ZVHI","inventory") %>% scale() %>% data.frame()
svm_df$SizeRank = svm_to_scale$SizeRank
svm_df$ZVHI = svm_to_scale$ZVHI
svm_df$inventory = svm_to_scale$inventory
svm_train = svm_df[train_ind,]
svm_df = df %>% select("y","SizeRank","ZVHI","inventory","season","loc_cluster","year")
svm_to_scale = svm_df %>% select("SizeRank","ZVHI","inventory") %>% scale() %>% data.frame()
svm_df$SizeRank = svm_to_scale$SizeRank
svm_df$ZVHI = svm_to_scale$ZVHI
svm_df$inventory = svm_to_scale$inventory
train_ind = sample(seq(nrow(df)), 0.2*nrow(df), replace = FALSE, prob = NULL)
svm_train = svm_df[train_ind,]
svm_test = svm_df[-train_ind,]
svm_df = df %>% select("y","SizeRank","ZVHI","inventory","season","loc_cluster","year")
svm_to_scale = svm_df %>% select("SizeRank","ZVHI","inventory") %>% scale() %>% data.frame()
svm_df$SizeRank = svm_to_scale$SizeRank
svm_df$ZVHI = svm_to_scale$ZVHI
svm_df$inventory = svm_to_scale$inventory
train_ind = sample(seq(nrow(df)), 0.8*nrow(df), replace = FALSE, prob = NULL)
svm_train = svm_df[train_ind,]
svm_test = svm_df[-train_ind,]
library(dplyr)
library(tidyverse)
library(mapview)
library(randomForest)
library(corrplot)
library(RColorBrewer)
library("PerformanceAnalytics")
library(car)
library(regclass)
library(e1071)
library(caret)
set.seed(1993)
df = read.csv("../../Data/datacleaning_plus_addedfeatures.csv")
svm_df = df %>% select("y","SizeRank","ZVHI","inventory","season","loc_cluster","year")
svm_to_scale = svm_df %>% select("SizeRank","ZVHI","inventory") %>% scale() %>% data.frame()
svm_df$SizeRank = svm_to_scale$SizeRank
svm_df$ZVHI = svm_to_scale$ZVHI
svm_df$inventory = svm_to_scale$inventory
train_ind = sample(seq(nrow(df)), 0.8*nrow(df), replace = FALSE, prob = NULL)
svm_train = svm_df[train_ind,]
svm_test = svm_df[-train_ind,]
modelsvm = svm(y~SizeRank+ZVHI+inventory+as.factor(season)+as.factor(loc_cluster)+year,svm_train)
#Predict using SVM regression
predYsvm = predict(modelsvm, svm_test)
#Overlay SVM Predictions on Scatter Plot
plot(svm_test$y-predYsvm, predYsvm)
sqrt(mean((svm_test$y-predYsvm)^2))
set.seed(1993)
val_ind = sample(seq(nrow(svm_train)), 0.2*nrow(svm_train), replace = FALSE, prob = NULL)
val_set = svm_train[val_ind,]
train_set = svm_train[-val_ind,]
kernels = c("linear","polynomial","radial","sigmoid")
RMSEs = c()
for(kernel in kernels){
modelsvm = svm(y~SizeRank+ZVHI+inventory+as.factor(season)+as.factor(loc_cluster)+year,kernel = kernel,data=train_set)
predYsvm = predict(modelsvm, val_set)
RMSEs = cbind(RMSEs,sqrt(mean((val_set$y-predYsvm)^2)))
}
knitr::opts_chunk$set(echo = TRUE)
# Clear the environment
rm(list=ls())
# Set a random seed for code reproducibility
set.seed(25)
# Install libraries
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(randomForest))
suppressMessages(library(Metrics))
suppressMessages(library(caret))
suppressMessages(library(mapview))
# Read the dataset
data <- read.csv('../../Data/datacleaning_plus_addedfeatures.csv', header=TRUE)
# View dataset
glimpse(data)
# Drop 'X.1' and 'X'
data <- data[, -which(names(data) %in% c('X.1', 'X'))]
# View data
glimpse(data)
# Randomly split data into training and testing sets
my_sample <- sample(x = nrow(data),
size = floor(nrow(data)*0.7),
replace = FALSE,
prob = NULL)
# Training data
train_data <- data[my_sample, ]
# Testing data
test_data <- data[-my_sample, ]
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(car)
library(tidyverse)
library(broom)
library(gridExtra)
# Reading the formatted dataframe
df <- read.csv("../../Data/datacleaning_plus_addedfeatures.csv")
head(df)
standard_model = lm(1/(y+1)~log(SizeRank)*log(ZVHI)+log(inventory)+as.factor(season)+as.factor(loc_cluster)+as.factor(year), data=df)
summary(standard_model)
model.diag.metrics <- augment(standard_model)
head(model.diag.metrics)
residplot1 <- ggplot(model.diag.metrics, aes(`log(ZVHI)`, `1/(y + 1)`)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = `log(ZVHI)`, yend = .fitted), color = "red", size = 0.1, alpha = 0.2)
residplot2 <- ggplot(model.diag.metrics, aes(`log(SizeRank)`, `1/(y + 1)`)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = `log(SizeRank)`, yend = .fitted), color = "red", size = 0.1, alpha = 0.2)
residplot3 <- ggplot(model.diag.metrics, aes(`log(inventory)`, `1/(y + 1)`)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = `log(inventory)`, yend = .fitted), color = "red", size = 0.1, alpha = 0.2)
grid.arrange(residplot1, residplot2, residplot3, nrow = 2, ncol = 2)
par(mfrow = c(2, 2))
plot(standard_model)
par(mfrow = c(1, 2))
plot(standard_model, 4, id.n = 5)
